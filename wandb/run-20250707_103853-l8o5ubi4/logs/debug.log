2025-07-07 10:38:53,228 INFO    MainThread:9368 [wandb_setup.py:_flush():80] Current SDK version is 0.21.0
2025-07-07 10:38:53,228 INFO    MainThread:9368 [wandb_setup.py:_flush():80] Configure stats pid to 9368
2025-07-07 10:38:53,229 INFO    MainThread:9368 [wandb_setup.py:_flush():80] Loading settings from C:\Users\90546\.config\wandb\settings
2025-07-07 10:38:53,229 INFO    MainThread:9368 [wandb_setup.py:_flush():80] Loading settings from C:\Users\90546\Desktop\customenv\wandb\settings
2025-07-07 10:38:53,229 INFO    MainThread:9368 [wandb_setup.py:_flush():80] Loading settings from environment variables
2025-07-07 10:38:53,229 INFO    MainThread:9368 [wandb_init.py:setup_run_log_directory():703] Logging user logs to C:\Users\90546\Desktop\customenv\wandb\run-20250707_103853-l8o5ubi4\logs\debug.log
2025-07-07 10:38:53,272 INFO    MainThread:9368 [wandb_init.py:setup_run_log_directory():704] Logging internal logs to C:\Users\90546\Desktop\customenv\wandb\run-20250707_103853-l8o5ubi4\logs\debug-internal.log
2025-07-07 10:38:53,272 INFO    MainThread:9368 [wandb_init.py:init():830] calling init triggers
2025-07-07 10:38:53,272 INFO    MainThread:9368 [wandb_init.py:init():835] wandb.init called with sweep_config: {}
config: {'algo': 'PPO', 'timesteps': 300000, 'reward_shaping': 'action_penalty + velocity_penalty + precision_bonus', 'env': 'Custom Reacher', 'notes': 'Baseline with action+velocity penalties', '_wandb': {}}
2025-07-07 10:38:53,272 INFO    MainThread:9368 [wandb_init.py:init():871] starting backend
2025-07-07 10:38:53,896 INFO    MainThread:9368 [wandb_init.py:init():874] sending inform_init request
2025-07-07 10:38:53,930 INFO    MainThread:9368 [wandb_init.py:init():882] backend started and connected
2025-07-07 10:38:53,932 INFO    MainThread:9368 [wandb_init.py:init():953] updated telemetry
2025-07-07 10:38:54,017 INFO    MainThread:9368 [wandb_init.py:init():977] communicating run to backend with 90.0 second timeout
2025-07-07 10:38:55,068 INFO    MainThread:9368 [wandb_init.py:init():1029] starting run threads in backend
2025-07-07 10:38:55,192 INFO    MainThread:9368 [wandb_run.py:_console_start():2458] atexit reg
2025-07-07 10:38:55,193 INFO    MainThread:9368 [wandb_run.py:_redirect():2306] redirect: wrap_raw
2025-07-07 10:38:55,193 INFO    MainThread:9368 [wandb_run.py:_redirect():2375] Wrapping output streams.
2025-07-07 10:38:55,193 INFO    MainThread:9368 [wandb_run.py:_redirect():2398] Redirects installed.
2025-07-07 10:38:55,198 INFO    MainThread:9368 [wandb_init.py:init():1075] run started, returning control to user process
2025-07-07 10:38:57,987 INFO    MainThread:9368 [wandb_watch.py:_watch():70] Watching
2025-07-07 10:38:57,988 INFO    MainThread:9368 [wandb_run.py:_config_callback():1363] config_cb None None {'policy_class': "<class 'stable_baselines3.common.policies.ActorCriticPolicy'>", 'device': 'cpu', 'verbose': 1, 'policy_kwargs': '{}', 'num_timesteps': 0, '_total_timesteps': 300000, '_num_timesteps_at_start': 0, 'seed': 'None', 'action_noise': 'None', 'start_time': 1751877537978089000, 'learning_rate': 0.0003, 'tensorboard_log': './ppo_tensorboard/', '_last_obs': '[[0.07654211 0.0079327  0.         0.         1.594773   0.12906876\n  0.2        1.2        0.         0.2       ]]', '_last_episode_starts': '[ True]', '_last_original_obs': 'None', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_custom_logger': 'False', '_vec_normalize_env': 'None', 'observation_space': 'Box(-inf, inf, (10,), float32)', 'action_space': 'Box(-1.0, 1.0, (2,), float32)', 'n_envs': 1, 'n_steps': 2048, 'gamma': 0.99, 'gae_lambda': 0.95, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'rollout_buffer_class': "<class 'stable_baselines3.common.buffers.RolloutBuffer'>", 'rollout_buffer_kwargs': '{}', 'batch_size': 64, 'n_epochs': 10, 'clip_range': '<function get_schedule_fn.<locals>.<lambda> at 0x00000236DC036980>', 'clip_range_vf': 'None', 'normalize_advantage': 'True', 'target_kl': 'None', 'lr_schedule': '<function get_schedule_fn.<locals>.<lambda> at 0x00000236D7578E00>', 'rollout_buffer': '<stable_baselines3.common.buffers.RolloutBuffer object at 0x00000236D751AE40>', 'policy': 'ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (pi_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (vf_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n    (policy_net): Sequential(\n      (0): Linear(in_features=10, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n    )\n    (value_net): Sequential(\n      (0): Linear(in_features=10, out_features=64, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n      (3): Tanh()\n    )\n  )\n  (action_net): Linear(in_features=64, out_features=2, bias=True)\n  (value_net): Linear(in_features=64, out_features=1, bias=True)\n)', '_logger': '<stable_baselines3.common.logger.Logger object at 0x00000236D76BD400>'}
